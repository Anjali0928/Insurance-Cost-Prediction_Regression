#Insurance Cost Prediction

# 1) IMPORT NECESSARY LIBRARIES
import pandas as pd                                                    # For data loading and manipulation
from sklearn.model_selection import train_test_split                   # To split data into train/test sets
from sklearn.preprocessing import OneHotEncoder, StandardScaler        # To encode categories and scale numerics
from sklearn.compose import ColumnTransformer                          # To apply different transforms to different columns
from sklearn.pipeline import Pipeline                                  # To chain preprocessing + modeling steps
from sklearn.linear_model import LinearRegression, Ridge, Lasso        # The three regression models
from sklearn.metrics import mean_squared_error, r2_score               # To evaluate model performance

# 2) LOAD THE DATASET
df = pd.read_csv("/home/insurance.csv") #https://drive.google.com/file/d/1d1XRpsFEaGKW5jr3XOB3nTUjUwYzII_i/view?usp=sharing
# insurance.csv has columns: age, sex, bmi, children, smoker, region, charges
print(df.head())  # Show the first 5 rows so you can see what the data looks like

# 3) DEFINE FEATURES (X) AND TARGET (y)
features = ["age", "sex", "bmi", "children", "smoker", "region"]
target   = "charges"

X = df[features]    # DataFrame of shape (1338, 6)
y = df[target]      # Series of length 1338

# 4) TRAIN-TEST SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,     # 20% for testing, 80% for training
    random_state=47    # fixed seed to make results reproducible
)

# 5) PREPROCESSING: ENCODE CATEGORICALS & SCALE NUMERICS
numeric_features   = ["age", "bmi", "children"]
categorical_feats  = ["sex", "smoker", "region"]

# ColumnTransformer applies different transforms to different columns
preprocessor = ColumnTransformer([
    ("num", StandardScaler(), numeric_features),              # scale numeric to mean=0, std=1
    ("cat", OneHotEncoder(drop="first", sparse_output=False), categorical_feats)
])

# 6) SET UP PIPES FOR EACH MODEL
# We’ll create three pipelines that first run `preprocessor`, then fit a regression.

lin_pipe = Pipeline([
    ("prep", preprocessor),
    ("lin", LinearRegression())
])

ridge_pipe = Pipeline([
    ("prep", preprocessor),
    ("ridge", Ridge(alpha=1.0))   # alpha=1.0 for moderate L2 regularization
])

lasso_pipe = Pipeline([
    ("prep", preprocessor),
    ("lasso", Lasso(alpha=0.1))   # alpha=0.1 for L1 regularization and feature selection
])

# 7) TRAIN EACH MODEL
lin_pipe.fit(X_train, y_train)        # trains preprocessing + linear regression
ridge_pipe.fit(X_train, y_train)      # trains preprocessing + ridge regression
lasso_pipe.fit(X_train, y_train)      # trains preprocessing + lasso regression

# 8) MAKE PREDICTIONS ON TEST SET
y_pred_lin   = lin_pipe.predict(X_test)
y_pred_ridge = ridge_pipe.predict(X_test)
y_pred_lasso = lasso_pipe.predict(X_test)

# 9) EVALUATE PERFORMANCE WITH RMSE & R²
for name, y_pred in [
    ("Linear Regression", y_pred_lin),
    ("Ridge Regression",  y_pred_ridge),
    ("Lasso Regression",  y_pred_lasso),
]:

    mse  = mean_squared_error(y_test, y_pred)  # average squared difference
    rmse = mse ** 0.5                           # root of MSE for same units as target
    r2   = r2_score(y_test, y_pred)                            # R-squared score
    print(f"{name:20s} -> RMSE: {rmse:.2f}, R²: {r2:.2f}")
